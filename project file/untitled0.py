# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-3ut1lQpL4Su4V3mEuOXm3iWxmGzEF0q
"""

import gradio as gr
from hf_model import classify_request
from ibm_granite import query_ibm_granite

def process(text, use_granite):
    if use_granite:
        return query_ibm_granite(text)
    else:
        return f"Predicted Service Type: {classify_request(text)}"

demo = gr.Interface(
    fn=process,
    inputs=[
        gr.Textbox(label="Describe the issue with your vehicle"),
        gr.Checkbox(label="Use IBM Granite instead of Hugging Face")
    ],
    outputs="text",
    title="Garage Management System AI Assistant",
    description="Predict service type or generate responses using IBM Granite"
)

if __name__ == "__main__":
    demo.launch()

pip install gradio transformers torch requests

# app.py

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import gradio as gr

# Load the IBM Granite model
model_id = "ibm-granite/granite-3.3-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# Create the pipeline
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Define the function to interact with the AI
def garage_ai_assistant(prompt):
    """
    Generates a response from the AI model based on the given prompt.
    """
    response = pipe(prompt, max_new_tokens=150, do_sample=True, temperature=0.7)[0]["generated_text"]
    return response

# Create the Gradio interface
interface = gr.Interface(
    fn=garage_ai_assistant,
    inputs=gr.Textbox(lines=2, placeholder="Describe the issue or ask about maintenance..."),
    outputs="text",
    title="Garage Management AI Assistant",
    description="""
    ðŸ”§ Ask questions about vehicle service, repairs, parts, or maintenance.\n
    ðŸ›  Example: 'My car makes a knocking sound when I accelerate.'\n
    ðŸ’¡ Powered by IBM Granite AI model.
    """,
    theme="default"
)

# Launch the app
if__name__=="__main__":
  interface.launch()

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import gradio as gr

# âœ… Load IBM Granite Model
model_id = "ibm-granite/granite-3.3-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

# âœ… Define Assistant Function
def garage_ai_assistant(prompt):
    response = pipe(prompt, max_new_tokens=150, do_sample=True, temperature=0.7)[0]["generated_text"]
    return response

# âœ… Launch Gradio App (with public sharing)
gr.Interface(
    fn=garage_ai_assistant,
    inputs=gr.Textbox(lines=2, placeholder="Describe the issue or ask about maintenance..."),
    outputs="text",
    title="Garage Management AI Assistant",
    description="ðŸ›  Ask about repairs, parts, service intervals, or noise diagnosis.\nðŸ”§ Powered by IBM Granite.",
).launch(share=True)

pip install transformers accelerate gradio